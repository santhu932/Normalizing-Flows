{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import math\n",
    "\n",
    "def logdet(L):\n",
    "    \"\"\"\n",
    "    Calculate the log determinant of L L^T.\n",
    "    \"\"\"\n",
    "    return (L.diag() ** 2).log().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
      "        0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
      "        1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
      "        0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
      "        1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
      "        1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0.,\n",
      "        1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
      "        1., 1.])\n",
      "tensor([0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
      "        0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# Create a very simple dataset\n",
    "torch.manual_seed(0)\n",
    "d = 5\n",
    "Phi = torch.randn(d, N)\n",
    "coeff = torch.ones(d)\n",
    "ts = torch.where((coeff @ Phi > 0.), torch.tensor(1.), torch.tensor(0.))\n",
    "print(ts)\n",
    "\n",
    "# For test\n",
    "Ntest = 50\n",
    "Phi_test = torch.randn(d, Ntest)\n",
    "ts_test = torch.where((coeff @ Phi_test > 0.), torch.tensor(1.), torch.tensor(0.))\n",
    "print(ts_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe the model and calculate the ELBO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Description: $p(w)=\\mathcal{N}(0, \\frac{1}{\\alpha} I), y_i = \\sigma(w^T \\phi(x_i))$ and observation $t_i\\sim \\text{Bernoulli}(y_i)=y_i^{t_i} (1-y_i)^{1-t_i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using $q(w)=\\mathcal{N}(m, LL^T)$ as our variational distribution, where $L$ is a lower triangular matrix, we can calculate the ELBO as below:\n",
    "\\begin{align*}\n",
    "\\text{ELBO}&=E_{q(w)}[\\ln p(w) - \\ln q(w) + \\ln p(t|w)] \\\\\n",
    "&=E_{q(w)}[(\\frac{d}{2} \\ln \\alpha - \\frac{\\alpha}{2} w^T w) + (\\frac{1}{2} \\ln \\det(L L^T) + \\frac{1}{2} (w-m)^T L^{-T} L^{-1} (w-m)) + (\\sum_i t_i \\ln y_i + (1-t_i)\\ln(1-y_i))].\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the function below to evaluate the quality of $q(w)$. It returns $\\log E_{q(w)}[p(t|w)]$ and the misclassification rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(m, L, Phi_test, ts_test):\n",
    "    mu = m @ Phi_test\n",
    "    var = torch.sum((L@Phi_test)**2, 0)\n",
    "    kappa = (1 + math.pi * var / 8) ** (-0.5)\n",
    "    \n",
    "    loss_func = torch.nn.BCEWithLogitsLoss()\n",
    "    log_loss = loss_func(kappa * mu, ts_test)\n",
    "    \n",
    "    prob = torch.sigmoid(kappa * mu)\n",
    "    prediction = (prob > 0.5).float()\n",
    "    misclassification = torch.sum((prediction != ts_test).int()).item() / ts_test.numel()\n",
    "    return log_loss, misclassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reparametrization trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $w=m+L \\epsilon, \\epsilon \\sim \\mathcal{N}(0, I)$. \"triangular_solve\" efficiently computes $L^{-1} A$ where $L$ is a lower triangular matrix and $A$ is any other matrix. A similar function called \"cholesky_solve\" computes $L^{-T} L^{-1} A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ELBO_reparametrization(alpha, m, L, Phi, ts, num_samples=1):\n",
    "    \"\"\"\n",
    "    Use reparametrization trick to compute ELBO.\n",
    "    :param alpha: prior parameter.\n",
    "    :param m: variational mean.\n",
    "    :param L: cholesky factor of variational covariance.\n",
    "    :param Phi: Phi matrix of xs. Each colomn represent a phi(x).\n",
    "    :param ts: observations.\n",
    "    \"\"\"\n",
    "    d = Phi.size(0)\n",
    "    \n",
    "    L = L.tril(0)\n",
    "    \n",
    "    eps = torch.randn(d, num_samples)\n",
    "    ws = m[:, None] + (L @ eps)  # ws is now of size (d, num_samples)\n",
    "    \n",
    "    lnp = d * torch.log(alpha) /2 - alpha * (ws **2).sum(0) / 2\n",
    "    \n",
    "    L_inv_w_m = torch.triangular_solve(ws - m[:, None], L, upper=False)[0]\n",
    "    lnq = logdet(L) / 2 + (L_inv_w_m ** 2).sum(0) / 2\n",
    "    \n",
    "    ys = torch.sigmoid(ws.t() @ Phi)\n",
    "    logl = (ts * torch.log(ys) + (1 - ts) * torch.log(1-ys)).sum(1)\n",
    "    \n",
    "    return (lnp + lnq + logl).sum() / num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the result on the simple dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0009 Loss:  129.097626 Test Performance:  0.390509  0.0\n",
      "Iteration: 0019 Loss:  64.714828 Test Performance:  0.215732  0.0\n",
      "Iteration: 0029 Loss:  56.918682 Test Performance:  0.158729  0.0\n",
      "Iteration: 0039 Loss:  50.749977 Test Performance:  0.138594  0.0\n",
      "Iteration: 0049 Loss:  47.075630 Test Performance:  0.126056  0.0\n",
      "Iteration: 0059 Loss:  45.723824 Test Performance:  0.116105  0.0\n",
      "Iteration: 0069 Loss:  45.048424 Test Performance:  0.108743  0.0\n",
      "Iteration: 0079 Loss:  44.142651 Test Performance:  0.103115  0.0\n",
      "Iteration: 0089 Loss:  43.614021 Test Performance:  0.099320  0.0\n",
      "Iteration: 0099 Loss:  43.450909 Test Performance:  0.096284  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_d/xh5shz5d66jfj6bn094jst6c0000gn/T/ipykernel_58129/2043495800.py:19: UserWarning: torch.triangular_solve is deprecated in favor of torch.linalg.solve_triangularand will be removed in a future PyTorch release.\n",
      "torch.linalg.solve_triangular has its arguments reversed and does not return a copy of one of the inputs.\n",
      "X = torch.triangular_solve(B, A).solution\n",
      "should be replaced with\n",
      "X = torch.linalg.solve_triangular(A, B). (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1678402353079/work/aten/src/ATen/native/BatchLinearAlgebra.cpp:2197.)\n",
      "  L_inv_w_m = torch.triangular_solve(ws - m[:, None], L, upper=False)[0]\n"
     ]
    }
   ],
   "source": [
    "num_samples = 10\n",
    "alpha = torch.tensor(1.)\n",
    "dim = 5\n",
    "\n",
    "m = torch.zeros(dim, requires_grad=True)\n",
    "L = torch.eye(dim).clone().detach().requires_grad_(True)\n",
    "\n",
    "optimizer = torch.optim.Adam([m, L], lr=0.05)\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    loss = -ELBO_reparametrization(alpha, m, L, Phi, ts, num_samples=num_samples)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # evaluate\n",
    "    with torch.no_grad():\n",
    "        logloss, mis = evaluate(m, L, Phi_test, ts_test)\n",
    "    if i % 10 == 9:\n",
    "        print('Iteration: {0:04d} Loss: {1: .6f} Test Performance: {2: .6f} {3: .1f}'.format(i, loss.item(), logloss.item(), mis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Black box variational inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Black Box variational inference uses $f_{\\lambda}(z)=\\nabla_{\\lambda} \\log q(z|\\lambda) (\\log p(x|z) + \\log p(z) - \\log q(z))$ to compute the gradients of ELBO. An easier way to implement is use the autograd framework to compute the $\\nabla_\\lambda \\log q(z|\\lambda)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BBVIAuto(m, L, alpha, Phi, ts, num_samples=5):\n",
    "    L = L.tril(0)\n",
    "    q_dist = torch.distributions.multivariate_normal.MultivariateNormal(loc=m, scale_tril=L)\n",
    "    p_dist = torch.distributions.multivariate_normal.MultivariateNormal(loc=torch.zeros_like(m), \\\n",
    "                                                                        covariance_matrix=1/alpha * torch.eye(m.numel()))\n",
    "    samples = q_dist.sample((num_samples,)) # of size (num_samples, d)\n",
    "    \n",
    "    lnp = p_dist.log_prob(samples)\n",
    "    lnq = q_dist.log_prob(samples)\n",
    "        \n",
    "    # Calculate log likelihood. Use BCEWithLogitsLoss for numerical stability.\n",
    "    loss_func = torch.nn.BCEWithLogitsLoss(reduction='none')\n",
    "    logl = -loss_func(samples @ Phi, ts.repeat(num_samples, 1)).sum(1)\n",
    "    \n",
    "    # We do not want to take the gradients of log q(z) inside the parentheses, so we detach it.\n",
    "    value = lnp + logl - lnq.detach().clone()\n",
    "    return (value * lnq).sum() / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0019 Loss: -1270.076172 Test Performance:  0.589947  0.0\n",
      "Iteration: 0039 Loss: -947.644287 Test Performance:  0.515785  0.0\n",
      "Iteration: 0059 Loss: -799.911499 Test Performance:  0.455023  0.0\n",
      "Iteration: 0079 Loss: -609.904663 Test Performance:  0.395307  0.0\n",
      "Iteration: 0099 Loss: -486.943512 Test Performance:  0.349302  0.0\n",
      "Iteration: 0119 Loss: -448.278015 Test Performance:  0.312564  0.0\n",
      "Iteration: 0139 Loss: -255.392273 Test Performance:  0.281208  0.0\n",
      "Iteration: 0159 Loss: -232.433105 Test Performance:  0.258804  0.0\n",
      "Iteration: 0179 Loss: -201.783279 Test Performance:  0.238110  0.0\n",
      "Iteration: 0199 Loss: -175.608307 Test Performance:  0.224246  0.0\n",
      "Iteration: 0219 Loss: -123.408623 Test Performance:  0.212608  0.0\n",
      "Iteration: 0239 Loss: -151.047577 Test Performance:  0.200374  0.0\n",
      "Iteration: 0259 Loss: -105.120094 Test Performance:  0.189645  0.0\n",
      "Iteration: 0279 Loss: -83.250877 Test Performance:  0.178871  0.0\n",
      "Iteration: 0299 Loss: -83.944405 Test Performance:  0.169706  0.0\n",
      "Iteration: 0319 Loss: -64.679642 Test Performance:  0.161507  0.0\n",
      "Iteration: 0339 Loss: -9.548932 Test Performance:  0.154102  0.0\n",
      "Iteration: 0359 Loss: -26.491917 Test Performance:  0.151478  0.0\n",
      "Iteration: 0379 Loss: -42.249149 Test Performance:  0.148976  0.0\n",
      "Iteration: 0399 Loss: -5.790667 Test Performance:  0.147910  0.0\n",
      "Iteration: 0419 Loss:  18.768705 Test Performance:  0.145290  0.0\n",
      "Iteration: 0439 Loss:  20.591143 Test Performance:  0.143606  0.0\n",
      "Iteration: 0459 Loss:  8.942065 Test Performance:  0.136699  0.0\n",
      "Iteration: 0479 Loss:  31.802700 Test Performance:  0.132946  0.0\n",
      "Iteration: 0499 Loss:  4.814526 Test Performance:  0.130082  0.0\n"
     ]
    }
   ],
   "source": [
    "num_samples = 50\n",
    "alpha = torch.tensor(1.)\n",
    "dim = 5\n",
    "\n",
    "m = torch.zeros(dim, requires_grad=True)\n",
    "L = torch.eye(dim).clone().detach().requires_grad_(True)\n",
    "\n",
    "optimizer = torch.optim.Adam([m, L], lr=0.01)\n",
    "for i in range(500):\n",
    "    optimizer.zero_grad()\n",
    "    loss = -BBVIAuto(m, L, alpha, Phi, ts, num_samples)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # evaluate\n",
    "    with torch.no_grad():\n",
    "        logloss, mis = evaluate(m, L, Phi_test, ts_test)\n",
    "    if i % 20 == 19:\n",
    "        print('Iteration: {0:04d} Loss: {1: .6f} Test Performance: {2: .6f} {3: .1f}'.format(i, loss.item(), logloss.item(), mis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, using the autograd framework in pytorch gives us no access to the gradients and thus it is hard to add the control variates. Instead, we can define our own loss function along with the backward method. The value returned by forward function is not important but we have to calculate the gradients inside the forward function and save them in \"context\". Then in backward function, we extract gradients from context and return them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that $f_{\\lambda}(z)=\\nabla_{\\lambda} \\log q(z|\\lambda) (\\log p(x|z) + \\log p(z) - \\log q(z))$ is the gradient and $h(z)=\\nabla_\\lambda \\log q(z|\\lambda)$ is the control variate. We have to manually calculate $\\nabla_m \\log q(z|m, L) = L^{-T} L^{-1} (z - m) $ and $\\nabla_L \\log q(z|m, L) = -L^{-T} + L^{-T} L^{-1} (z-m)(z-m)^T L^{-T}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BBVILoss(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, m, L, alpha, Phi, ts, num_samples=5):\n",
    "        L = L.tril(0)\n",
    "        q_dist = torch.distributions.multivariate_normal.MultivariateNormal(loc=m, scale_tril=L)\n",
    "        p_dist = torch.distributions.multivariate_normal.MultivariateNormal(loc=torch.zeros_like(m), \\\n",
    "                                                                            covariance_matrix=1/alpha * torch.eye(m.numel()))\n",
    "        samples = q_dist.sample((num_samples,)) # of size (num_samples, d)\n",
    "        \n",
    "        lnp = p_dist.log_prob(samples)\n",
    "        lnq = q_dist.log_prob(samples)\n",
    "        \n",
    "        # Calculate log likelihood. Use BCEWithLogitsLoss for numerical stability.\n",
    "        loss_func = torch.nn.BCEWithLogitsLoss(reduction='none')\n",
    "        logl = -loss_func(samples @ Phi, ts.repeat(num_samples, 1)).sum(1)\n",
    "        value = lnp + logl - lnq\n",
    "        \n",
    "        dlnq_dm = torch.zeros(num_samples, d)\n",
    "        dlnq_dL = torch.zeros(num_samples, d, d)\n",
    "        L_inv_T = (torch.triangular_solve(torch.eye(L.size(0)), L)[0]).t()\n",
    "        for i in range(num_samples):\n",
    "            temp = (samples[i] - m).unsqueeze(-1)\n",
    "            dlnq_dm[i] = (torch.cholesky_solve(temp, L)).squeeze()\n",
    "            dlnq_dL[i] = -L_inv_T + torch.cholesky_solve(temp @ temp.t() @ L_inv_T, L)\n",
    "        \n",
    "        # calculate 'a' for m\n",
    "        fm = value[:, None] * dlnq_dm\n",
    "        hm = dlnq_dm\n",
    "        fm_mean = fm.sum(0) / num_samples\n",
    "        hm_mean = hm.sum(0) / num_samples\n",
    "        cov_fm_hm = ((fm-fm_mean)*hm).sum(0) / (num_samples - 1)\n",
    "        var_hm = (hm**2).sum(0) / (num_samples - 1)\n",
    "        am = cov_fm_hm / var_hm\n",
    "        \n",
    "        # calculate 'a' for L\n",
    "        fL = value[:, None, None] * dlnq_dL\n",
    "        hL = dlnq_dL\n",
    "        fL_mean = fL.sum(0) / num_samples\n",
    "        hL_mean = hL.sum(0) / num_samples\n",
    "        cov_fL_hL = ((fL-fL_mean)*(hL-hL_mean)).sum(0) / (num_samples - 1)\n",
    "        var_hL = ((hL-hL_mean)**2).sum(0) / (num_samples - 1)\n",
    "        aL = cov_fL_hL / var_hL\n",
    "        \n",
    "        delbo_dm = (value[:, None] * dlnq_dm - am[None, :] * dlnq_dm).sum(0).div(num_samples)\n",
    "        delbo_dL = (value[:, None, None]* dlnq_dL - aL[None, :, :] * dlnq_dL).sum(0).div(num_samples)\n",
    "        \n",
    "        ctx.save_for_backward(delbo_dm, delbo_dL)\n",
    "        return (lnp + logl - lnq).sum().div(num_samples)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        grad_m = grad_L = None\n",
    "        delbo_dm, delbo_dL = ctx.saved_tensors\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_m = grad_output * delbo_dm\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_L = grad_output * delbo_dL\n",
    "        return grad_m, grad_L, None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0019 Loss:  170.104980 Test Performance:  0.584444  0.0\n",
      "Iteration: 0039 Loss:  134.841248 Test Performance:  0.476614  0.0\n",
      "Iteration: 0059 Loss:  104.949593 Test Performance:  0.390326  0.0\n",
      "Iteration: 0079 Loss:  90.097000 Test Performance:  0.322188  0.0\n",
      "Iteration: 0099 Loss:  75.086990 Test Performance:  0.269097  0.0\n",
      "Iteration: 0119 Loss:  67.526443 Test Performance:  0.236580  0.0\n",
      "Iteration: 0139 Loss:  63.855225 Test Performance:  0.216595  0.0\n",
      "Iteration: 0159 Loss:  60.096058 Test Performance:  0.200095  0.0\n",
      "Iteration: 0179 Loss:  57.848900 Test Performance:  0.186758  0.0\n",
      "Iteration: 0199 Loss:  55.048630 Test Performance:  0.173956  0.0\n",
      "Iteration: 0219 Loss:  53.447769 Test Performance:  0.165810  0.0\n",
      "Iteration: 0239 Loss:  53.225605 Test Performance:  0.159034  0.0\n",
      "Iteration: 0259 Loss:  51.165985 Test Performance:  0.152136  0.0\n",
      "Iteration: 0279 Loss:  50.513100 Test Performance:  0.146309  0.0\n",
      "Iteration: 0299 Loss:  50.353901 Test Performance:  0.140852  0.0\n",
      "Iteration: 0319 Loss:  48.364677 Test Performance:  0.136010  0.0\n",
      "Iteration: 0339 Loss:  48.347237 Test Performance:  0.132122  0.0\n",
      "Iteration: 0359 Loss:  48.057545 Test Performance:  0.128876  0.0\n",
      "Iteration: 0379 Loss:  48.098179 Test Performance:  0.126010  0.0\n",
      "Iteration: 0399 Loss:  46.996552 Test Performance:  0.122501  0.0\n",
      "Iteration: 0419 Loss:  46.097233 Test Performance:  0.119320  0.0\n",
      "Iteration: 0439 Loss:  46.164482 Test Performance:  0.116739  0.0\n",
      "Iteration: 0459 Loss:  45.042191 Test Performance:  0.114928  0.0\n",
      "Iteration: 0479 Loss:  45.178204 Test Performance:  0.113798  0.0\n",
      "Iteration: 0499 Loss:  44.550095 Test Performance:  0.110822  0.0\n"
     ]
    }
   ],
   "source": [
    "num_samples = 50\n",
    "alpha = torch.tensor(1.)\n",
    "dim = 5\n",
    "\n",
    "m = torch.zeros(dim, requires_grad=True)\n",
    "L = torch.eye(dim).clone().detach().requires_grad_(True)\n",
    "\n",
    "optimizer = torch.optim.Adam([m, L], lr=0.01)\n",
    "for i in range(500):\n",
    "    optimizer.zero_grad()\n",
    "    loss = -BBVILoss.apply(m, L, alpha, Phi, ts, num_samples)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # evaluate\n",
    "    with torch.no_grad():\n",
    "        logloss, mis = evaluate(m, L, Phi_test, ts_test)\n",
    "    if i % 20 == 19:\n",
    "        print('Iteration: {0:04d} Loss: {1: .6f} Test Performance: {2: .6f} {3: .1f}'.format(i, loss.item(), logloss.item(), mis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
